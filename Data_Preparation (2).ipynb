{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**DATA ACUISITION and MANIPULATION**"
      ],
      "metadata": {
        "id": "JL03tkADZDkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IxFF_MQQDyv",
        "outputId": "740b4c1b-b9b1-456c-adac-15ac49f80d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting only US States data from the world Influenza-like-Illness(ILI) dataset and merging it with December 2020 pollutants data"
      ],
      "metadata": {
        "id": "OTenUapMfB9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "\n",
        "# 1. Load Data\n",
        "ilin_df = pd.read_csv(\"/content/drive/MyDrive/ILINet.csv\", header=1)\n",
        "aq_df = pd.read_csv(\"/content/drive/MyDrive/forecasts_Dec_2020_df.csv\")\n",
        "\n",
        "# 2. Prepare and Expand ILINet Data to Daily Rows (Target + Temporal Features)\n",
        "# Converting ~8,500 weekly records to ~60,000 daily records.\n",
        "ilin_df = ilin_df[ilin_df['REGION TYPE'] == 'States'].copy()\n",
        "ilin_df.rename(columns={'REGION': 'state_name'}, inplace=True)\n",
        "\n",
        "# Clean up ILI target variable\n",
        "ilin_df.replace('X', np.nan, inplace=True)\n",
        "ilin_df['%UNWEIGHTED ILI'] = pd.to_numeric(ilin_df['%UNWEIGHTED ILI'], errors='coerce')\n",
        "ilin_df.dropna(subset=['%UNWEIGHTED ILI'], inplace=True)\n",
        "ilin_df['state_name'] = ilin_df['state_name'].str.strip()\n",
        "\n",
        "# Create a weekly date for joining (Week Ending Date - Saturday)\n",
        "# Using pandas ISO calendar to create the date for the week *starting* on Monday of that week.\n",
        "def get_week_ending_date(row):\n",
        "    try:\n",
        "        # Create date object for Thursday of the given ISO year/week (mid-week)\n",
        "        # Using 4 for Thursday, as it is always in the specified week number\n",
        "        # Adjust to Saturday (Week Ending Date: +2 days)\n",
        "        return pd.to_datetime(f\"{int(row['YEAR'])}-{int(row['WEEK'])}-4\", format=\"%Y-%W-%w\") + timedelta(days=2)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "# Handle edge cases (Week 53) manually to ensure the correct year assignment\n",
        "ilin_df.loc[((ilin_df['WEEK'] == 53) & (ilin_df['YEAR'] == 2010)), 'WEEK'] = 52\n",
        "ilin_df.loc[((ilin_df['WEEK'] == 53) & (ilin_df['YEAR'] == 2021)), 'WEEK'] = 52\n",
        "ilin_df['WEEKEND'] = ilin_df.apply(get_week_ending_date, axis=1)\n",
        "\n",
        "# Backfill NaN dates for robustness by using the standard CDC convention:\n",
        "# Week 1 starts on the first Sunday of the year, and the week ends on the following Saturday.\n",
        "# A simpler method for creating the week ending date, which avoids the year/week/day ISO issues:\n",
        "# 1. Find the first day of the year/week.\n",
        "# 2. Add 6 days to get the Saturday end-date.\n",
        "ilin_df.dropna(subset=['WEEKEND'], inplace=True)\n",
        "ilin_df.rename(columns={'WEEKEND': 'Week Ending Date'}, inplace=True)\n",
        "\n",
        "# Convert weekly records to daily records (replication)\n",
        "daily_records = []\n",
        "for index, row in ilin_df.iterrows():\n",
        "    end_date = row['Week Ending Date']\n",
        "    start_date = end_date - timedelta(days=6)\n",
        "\n",
        "    # Replicate the weekly data for 7 days\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "    for date in dates:\n",
        "        daily_records.append({\n",
        "            'state_name': row['state_name'],\n",
        "            'date': date,\n",
        "            'Year_ili': row['YEAR'],\n",
        "            'Week_ili': row['WEEK'],\n",
        "            'Week Ending Date': end_date,\n",
        "            'ILI_Target': row['%UNWEIGHTED ILI'],\n",
        "            'Total_Patients': row['TOTAL PATIENTS'],\n",
        "            'Num_Providers': row['NUM. OF PROVIDERS']\n",
        "        })\n",
        "\n",
        "daily_ili_df = pd.DataFrame(daily_records)\n",
        "daily_ili_df['state_name'] = daily_ili_df['state_name'].str.strip()\n",
        "\n",
        "\n",
        "# --- 3. Prepare AQ Daily Data (Features) ---\n",
        "# Aggregate county-level AQ data to state-level averages to match the ILINet data\n",
        "aq_df['date'] = pd.to_datetime(aq_df['date'])\n",
        "aq_df['state_name'] = aq_df['state_name'].str.strip()\n",
        "\n",
        "aq_cols = ['co_polutant_level', 'no2_polutant_level', 'o3_polutant_level',\n",
        "           'pm10_polutant_level', 'pm25frm_polutant_level', 'pm25nfrm_polutant_level',\n",
        "           'so2_polutant_level']\n",
        "for col in aq_cols:\n",
        "    aq_df[col] = pd.to_numeric(aq_df[col], errors='coerce')\n",
        "\n",
        "# Aggregate to state-level daily means\n",
        "aq_daily_state_avg = aq_df.groupby(['state_name', 'date'])[aq_cols].mean().reset_index()\n",
        "\n",
        "# --- 4. Merge Expanded ILINet Data (Target) with AQ Data (Features) ---\n",
        "# Left merge: Daily ILINet data is the core structure. AQ features are added where available (Dec 2020).\n",
        "final_merged_df = pd.merge(daily_ili_df, aq_daily_state_avg, on=['state_name', 'date'], how='left')\n",
        "\n",
        "# --- 5. Final Cleanup and Output ---\n",
        "final_merged_df['Day_of_Week_Name'] = final_merged_df['date'].dt.day_name()\n",
        "\n",
        "# Select final columns\n",
        "final_cols = [\n",
        "    'state_name', 'date', 'Day_of_Week_Name', 'Week Ending Date', 'Year_ili', 'Week_ili',\n",
        "    'ILI_Target',\n",
        "    'Total_Patients', 'Num_Providers',\n",
        "    'co_polutant_level', 'no2_polutant_level', 'o3_polutant_level', 'pm10_polutant_level',\n",
        "    'pm25frm_polutant_level', 'pm25nfrm_polutant_level', 'so2_polutant_level',\n",
        "]\n",
        "\n",
        "final_merged_df = final_merged_df[final_cols]\n",
        "\n",
        "# Save the final file\n",
        "output_file_name = \"full_daily_sparse_features_ili_target_US_states.csv\"\n",
        "final_merged_df.to_csv(output_file_name, index=False)\n",
        "print(f\"File created: {output_file_name}\")\n",
        "print(f\"Final Merged Dataset size (rows): {len(final_merged_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP3ErvsfzVHJ",
        "outputId": "e1630192-a9a3-438a-f4bc-1a485d720f14"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1891356651.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  ilin_df.replace('X', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created: full_daily_sparse_features_ili_target_US_states.csv\n",
            "Final Merged Dataset size (rows): 295659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the ILI data was being too large and complex and the pollutant data was sparse for continuous years, we decided to filter data from 2015 to 2025"
      ],
      "metadata": {
        "id": "1zsI9C9Nf47h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "\n",
        "# 1. Load the file\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/full_daily_sparse_features_ili_target_US_states.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file full_daily_sparse_features_ili_target_US_states.csv was not found.\")\n",
        "    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n",
        "\n",
        "# 2. Ensure 'date' is datetime and filter\n",
        "if not df.empty:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # 3. Filter data for 2015 to 2025 (inclusive)\n",
        "    # The upper limit is 2025, which is the latest data point available.\n",
        "    df_filtered = df[df['date'].dt.year >= 2015].copy()\n",
        "\n",
        "    # 4. Save the filtered DataFrame\n",
        "    output_file_name = \"full_daily_sparse_features_ili_target_US_states_2015_2025.csv\"\n",
        "    df_filtered.to_csv(output_file_name, index=False)\n",
        "\n",
        "    # Report size\n",
        "    print(f\"Filtered Dataset size (rows): {len(df_filtered)}\")\n",
        "    print(f\"File created: {output_file_name}\")\n",
        "else:\n",
        "    print(\"Cannot process the request because the source file could not be loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI2e_jLTq8JZ",
        "outputId": "f3b3703d-a9ae-4faa-98d9-799aa8cb89df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Dataset size (rows): 213739\n",
            "File created: full_daily_sparse_features_ili_target_US_states_2015_2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ILI data was completely manipulated but the AQ data was for December 2020 and Weather data was entirely missing. So after finding the merged AQ and weather data from the year 2015 to 2025 of US states, we merged it to the ILI data based on state and year"
      ],
      "metadata": {
        "id": "dbpSuOEwgbp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Load Main Daily File (2015‚Äì2025)\n",
        "main_df = pd.read_csv(\"/content/drive/MyDrive/full_daily_features_with_all_pollutants_2015_2025.csv\")\n",
        "\n",
        "main_df['date'] = pd.to_datetime(main_df['date'], errors='coerce')\n",
        "main_df['state_name'] = main_df['state_name'].str.strip()\n",
        "\n",
        "# Helper Function for Pollutant ID\n",
        "def get_pollutant_id(name):\n",
        "    if 'PM2.5' in name:\n",
        "        return 'PM2.5'\n",
        "    elif 'PM10' in name:\n",
        "        return 'PM10'\n",
        "    elif 'Light Absorption Coeffiecient' in name:\n",
        "        return 'Light_Absorption_Coeffiecient'\n",
        "    elif 'Average Ambient Temperature' in name:\n",
        "        return 'Average_Ambient_Temperature'\n",
        "    elif 'Average Ambient Pressure' in name:\n",
        "        return 'Average_Ambient_Pressure'\n",
        "    return 'Other_Max_Annual_Reading'\n",
        "\n",
        "# 2. Loop Through Multiple Years of Monitor Files\n",
        "all_years_df = []   # To store processed data for each year\n",
        "\n",
        "for year in range(2015, 2026):   # 2015 to 2025\n",
        "    file_path = f\"/content/drive/MyDrive/annual_conc_by_monitor_{year}.csv\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå File not found for {year}: {file_path}\")\n",
        "        continue\n",
        "    print(f\"‚úÖ Processing: {file_path}\")\n",
        "\n",
        "    # Load the file\n",
        "    monitor_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Rename columns\n",
        "    monitor_df.rename(columns={\n",
        "        'State Name': 'state_name',\n",
        "        '1st Max Value': 'Max_Value',\n",
        "        '1st Max DateTime': 'Max_DateTime'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Filter to keep only PM-related parameters\n",
        "    pm_parameters = monitor_df[\n",
        "        monitor_df['Parameter Name'].str.contains(\n",
        "            'PM2.5|PM10|Light Absorption Coeffiecient|Average Ambient Pressure|Average Ambient Temperature',\n",
        "            na=False\n",
        "        ) &\n",
        "        ~monitor_df['Parameter Name'].str.contains('NO2|SO2|CO|Ozone', na=False)\n",
        "    ].copy()\n",
        "\n",
        "    pm_parameters['state_name'] = pm_parameters['state_name'].str.strip()\n",
        "    pm_parameters['date'] = pd.to_datetime(pm_parameters['Max_DateTime'], errors='coerce').dt.date\n",
        "\n",
        "    # Pollutant ID column\n",
        "    pm_parameters['Pollutant_ID'] = pm_parameters['Parameter Name'].apply(get_pollutant_id)\n",
        "\n",
        "    # Group and compute max for each pollutant in each state & date\n",
        "    max_pm_pivot = pm_parameters.groupby(\n",
        "        ['state_name', 'date', 'Pollutant_ID']\n",
        "    )['Max_Value'].max().reset_index()\n",
        "\n",
        "    # Pivot to wide format\n",
        "    max_pm_final = max_pm_pivot.pivot_table(\n",
        "        index=['state_name', 'date'],\n",
        "        columns='Pollutant_ID',\n",
        "        values='Max_Value',\n",
        "        aggfunc='max'\n",
        "    ).reset_index()\n",
        "\n",
        "    max_pm_final['date'] = pd.to_datetime(max_pm_final['date'])\n",
        "\n",
        "    all_years_df.append(max_pm_final)\n",
        "\n",
        "# 3. Combine All Years\n",
        "if all_years_df:\n",
        "    combined_max_pm = pd.concat(all_years_df, ignore_index=True)\n",
        "else:\n",
        "    raise ValueError(\"No annual monitor files were successfully processed.\")\n",
        "\n",
        "print(\"üîÑ Combined annual monitor shape:\", combined_max_pm.shape)\n",
        "\n",
        "# 4. Merge With Main DF\n",
        "final_merged_df = pd.merge(\n",
        "    main_df,\n",
        "    combined_max_pm,\n",
        "    on=['state_name', 'date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 5. Save Final File\n",
        "output_file = \"/content/drive/MyDrive/full_daily_features_with_PM_max_2015_2025_FINAL.csv\"\n",
        "final_merged_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"üéâ File created:\", output_file)\n",
        "print(\"üìå Final rows:\", len(final_merged_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYXi9WUAiTFd",
        "outputId": "1b1271f4-e2c2-4770-c354-361bda92a056"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2015.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2016.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-808240213.py:45: DtypeWarning: Columns (38,40,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  monitor_df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2017.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2018.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2019.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2020.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2021.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2022.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2023.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2024.csv\n",
            "‚úÖ Processing: /content/drive/MyDrive/annual_conc_by_monitor_2025.csv\n",
            "üîÑ Combined annual monitor shape: (49523, 7)\n",
            "üéâ File created: /content/drive/MyDrive/full_daily_features_with_PM_max_2015_2025_FINAL.csv\n",
            "üìå Final rows: 213739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the final merged dataset"
      ],
      "metadata": {
        "id": "vfsy-PNnhmVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/daily_AQ_2015_2025.csv\")\n",
        "daily_cols = [\n",
        "    'PM2.5_pollutant_level', 'PM10_pollutant_level',\n",
        "    'carbon_monoxide_pollutant_level', 'ozone_pollutant_level',\n",
        "    'nitrogen_dioxide_pollutant_level', 'sulfur_dioxide_pollutant_level'\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d174b0n1y_o1",
        "outputId": "1c705009-1507-4f1a-ca99-49aae7cd1b0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         state_name        date Day_of_Week_Name Week Ending Date  Year_ili  \\\n",
            "0           Alabama  01-01-2015         Thursday       03-01-2015      2014   \n",
            "1           Alabama  02-01-2015           Friday       03-01-2015      2014   \n",
            "2           Alabama  03-01-2015         Saturday       03-01-2015      2014   \n",
            "3            Alaska  01-01-2015         Thursday       03-01-2015      2014   \n",
            "4            Alaska  02-01-2015           Friday       03-01-2015      2014   \n",
            "...             ...         ...              ...              ...       ...   \n",
            "213734  Puerto Rico  18-11-2025          Tuesday       22-11-2025      2025   \n",
            "213735  Puerto Rico  19-11-2025        Wednesday       22-11-2025      2025   \n",
            "213736  Puerto Rico  20-11-2025         Thursday       22-11-2025      2025   \n",
            "213737  Puerto Rico  21-11-2025           Friday       22-11-2025      2025   \n",
            "213738  Puerto Rico  22-11-2025         Saturday       22-11-2025      2025   \n",
            "\n",
            "        Week_ili  ILI_Target  Total_Patients  Num_Providers  \\\n",
            "0             52    19.41770            8827             24   \n",
            "1             52    19.41770            8827             24   \n",
            "2             52    19.41770            8827             24   \n",
            "3             52     2.34542             938              6   \n",
            "4             52     2.34542             938              6   \n",
            "...          ...         ...             ...            ...   \n",
            "213734        46     5.35567           17010             18   \n",
            "213735        46     5.35567           17010             18   \n",
            "213736        46     5.35567           17010             18   \n",
            "213737        46     5.35567           17010             18   \n",
            "213738        46     5.35567           17010             18   \n",
            "\n",
            "        ozone_pollutant_level  sulfur_dioxide_pollutant_level  \\\n",
            "0                       0.031                             3.0   \n",
            "1                       0.022                             0.7   \n",
            "2                       0.025                             0.9   \n",
            "3                       0.031                            13.2   \n",
            "4                       0.039                            16.4   \n",
            "...                       ...                             ...   \n",
            "213734                  0.017                             2.0   \n",
            "213735                  0.017                             2.0   \n",
            "213736                  0.017                             2.0   \n",
            "213737                  0.017                             2.0   \n",
            "213738                  0.017                             2.0   \n",
            "\n",
            "        carbon_monoxide_pollutant_level  nitrogen_dioxide_pollutant_level  \\\n",
            "0                                  0.71                              24.0   \n",
            "1                                  0.60                              27.2   \n",
            "2                                  0.42                              14.9   \n",
            "3                                  0.60                              32.1   \n",
            "4                                  2.40                              37.4   \n",
            "...                                 ...                               ...   \n",
            "213734                             0.90                              12.9   \n",
            "213735                             0.90                              12.9   \n",
            "213736                             0.90                              12.9   \n",
            "213737                             0.90                              12.9   \n",
            "213738                             0.90                              12.9   \n",
            "\n",
            "        Average_Ambient_Pressure  Average_Ambient_Temperature  \\\n",
            "0                            NaN                          NaN   \n",
            "1                            NaN                          NaN   \n",
            "2                            NaN                          NaN   \n",
            "3                            NaN                          NaN   \n",
            "4                            NaN                          NaN   \n",
            "...                          ...                          ...   \n",
            "213734                       NaN                          NaN   \n",
            "213735                       NaN                          NaN   \n",
            "213736                       NaN                          NaN   \n",
            "213737                       NaN                          NaN   \n",
            "213738                       NaN                          NaN   \n",
            "\n",
            "        PM10_pollutant_level  PM2.5_pollutant_level  \\\n",
            "0                       69.0               0.810500   \n",
            "1                       69.0               0.810500   \n",
            "2                       69.0               0.000000   \n",
            "3                       74.5               0.409255   \n",
            "4                       74.5               0.409255   \n",
            "...                      ...                    ...   \n",
            "213734                  75.5               3.513190   \n",
            "213735                  75.5               3.513190   \n",
            "213736                  75.5               3.513190   \n",
            "213737                  75.5               3.513190   \n",
            "213738                  75.5               3.513190   \n",
            "\n",
            "        Light_Absorption_Coeffiecient  \n",
            "0                                 NaN  \n",
            "1                                 NaN  \n",
            "2                                 NaN  \n",
            "3                                 NaN  \n",
            "4                                 NaN  \n",
            "...                               ...  \n",
            "213734                            NaN  \n",
            "213735                            NaN  \n",
            "213736                            NaN  \n",
            "213737                            NaN  \n",
            "213738                            NaN  \n",
            "\n",
            "[213739 rows x 18 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Missing Values"
      ],
      "metadata": {
        "id": "IOokXOGziZjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/daily_AQ_2015_2025.csv')\n",
        "\n",
        "# Convert date columns to datetime for sorting and potential time-based operations\n",
        "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
        "df['Week Ending Date'] = pd.to_datetime(df['Week Ending Date'], format='%d-%m-%Y', errors='coerce')\n",
        "\n",
        "# Sort by state and date to enable proper forward-fill\n",
        "df = df.sort_values(['state_name', 'date'])\n",
        "\n",
        "# Identify categorical columns for forward-fill\n",
        "categorical_cols = ['state_name', 'Day_of_Week_Name', 'Week Ending Date', 'Year_ili', 'Week_ili']\n",
        "\n",
        "# Identify daily pollutant columns for state-wise median imputation\n",
        "daily_pollutant_cols = [\n",
        "    'ozone_pollutant_level', 'sulfur_dioxide_pollutant_level', 'carbon_monoxide_pollutant_level',\n",
        "    'nitrogen_dioxide_pollutant_level', 'Average_Ambient_Pressure', 'Average_Ambient_Temperature',\n",
        "    'PM10_pollutant_level', 'PM2.5_pollutant_level', 'Light_Absorption_Coeffiecient'\n",
        "]\n",
        "\n",
        "# Other numerical columns (weekly health metrics, repeated daily; forward-fill as alternative if needed, but median here for consistency)\n",
        "other_numerical_cols = ['ILI_Target', 'Total_Patients', 'Num_Providers']\n",
        "\n",
        "# Add missing-value indicator flags for all imputable numerical columns\n",
        "imputable_cols = daily_pollutant_cols + other_numerical_cols\n",
        "for col in imputable_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[f'{col}_was_missing'] = df[col].isnull().astype(int)\n",
        "\n",
        "# Impute categorical columns with forward-fill within each state\n",
        "for col in categorical_cols:\n",
        "    df[col] = df.groupby('state_name')[col].ffill()\n",
        "\n",
        "# Impute daily pollutant columns with state-wise median\n",
        "for col in daily_pollutant_cols:\n",
        "    state_medians = df.groupby('state_name')[col].median()\n",
        "    df[col] = df.apply(\n",
        "        lambda row: state_medians.get(row['state_name'], np.nan) if pd.isnull(row[col]) else row[col],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# For other numerical (weekly) cols, use state-wise median as well (since they may vary less)\n",
        "for col in other_numerical_cols:\n",
        "    state_medians = df.groupby('state_name')[col].median()\n",
        "    df[col] = df.apply(\n",
        "        lambda row: state_medians.get(row['state_name'], np.nan) if pd.isnull(row[col]) else row[col],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# If any NaNs remain (e.g., entire state missing for a col), fall back to global median\n",
        "for col in imputable_cols:\n",
        "    if df[col].isnull().any():\n",
        "        global_median = df[col].median()\n",
        "        df[col] = df[col].fillna(global_median)\n",
        "\n",
        "# Verify no NaNs left\n",
        "print(\"Missing values after imputation:\\n\", df.isnull().sum())\n",
        "\n",
        "# Optionally save the imputed dataset\n",
        "df.to_csv('imputed_daily_AQ_2015_2025.csv', index=False)\n",
        "\n",
        "# Preview the imputed DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEQLZrqC-3T4",
        "outputId": "8cb27755-5e85-464e-e048-0b39d9634c27"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            " state_name                                      0\n",
            "date                                            0\n",
            "Day_of_Week_Name                                0\n",
            "Week Ending Date                                0\n",
            "Year_ili                                        0\n",
            "Week_ili                                        0\n",
            "ILI_Target                                      0\n",
            "Total_Patients                                  0\n",
            "Num_Providers                                   0\n",
            "ozone_pollutant_level                           0\n",
            "sulfur_dioxide_pollutant_level                  0\n",
            "carbon_monoxide_pollutant_level                 0\n",
            "nitrogen_dioxide_pollutant_level                0\n",
            "Average_Ambient_Pressure                        0\n",
            "Average_Ambient_Temperature                     0\n",
            "PM10_pollutant_level                            0\n",
            "PM2.5_pollutant_level                           0\n",
            "Light_Absorption_Coeffiecient                   0\n",
            "ozone_pollutant_level_was_missing               0\n",
            "sulfur_dioxide_pollutant_level_was_missing      0\n",
            "carbon_monoxide_pollutant_level_was_missing     0\n",
            "nitrogen_dioxide_pollutant_level_was_missing    0\n",
            "Average_Ambient_Pressure_was_missing            0\n",
            "Average_Ambient_Temperature_was_missing         0\n",
            "PM10_pollutant_level_was_missing                0\n",
            "PM2.5_pollutant_level_was_missing               0\n",
            "Light_Absorption_Coeffiecient_was_missing       0\n",
            "dtype: int64\n",
            "    state_name       date Day_of_Week_Name Week Ending Date  Year_ili  \\\n",
            "0      Alabama 2015-01-01         Thursday       2015-01-03      2014   \n",
            "1      Alabama 2015-01-02           Friday       2015-01-03      2014   \n",
            "2      Alabama 2015-01-03         Saturday       2015-01-03      2014   \n",
            "162    Alabama 2015-01-04           Sunday       2015-01-10      2014   \n",
            "540    Alabama 2015-01-04           Sunday       2015-01-10      2015   \n",
            "\n",
            "     Week_ili  ILI_Target  Total_Patients  Num_Providers  \\\n",
            "0          52    19.41770            8827             24   \n",
            "1          52    19.41770            8827             24   \n",
            "2          52    19.41770            8827             24   \n",
            "162        53    11.22120            8983             24   \n",
            "540         1     5.42916            9670             23   \n",
            "\n",
            "     ozone_pollutant_level  ...  Light_Absorption_Coeffiecient  \\\n",
            "0                    0.031  ...                        0.21665   \n",
            "1                    0.022  ...                        0.21665   \n",
            "2                    0.025  ...                        0.21665   \n",
            "162                  0.028  ...                        0.21665   \n",
            "540                  0.028  ...                        0.21665   \n",
            "\n",
            "     ozone_pollutant_level_was_missing  \\\n",
            "0                                    0   \n",
            "1                                    0   \n",
            "2                                    0   \n",
            "162                                  0   \n",
            "540                                  0   \n",
            "\n",
            "     sulfur_dioxide_pollutant_level_was_missing  \\\n",
            "0                                             0   \n",
            "1                                             0   \n",
            "2                                             0   \n",
            "162                                           0   \n",
            "540                                           0   \n",
            "\n",
            "     carbon_monoxide_pollutant_level_was_missing  \\\n",
            "0                                              0   \n",
            "1                                              0   \n",
            "2                                              0   \n",
            "162                                            0   \n",
            "540                                            0   \n",
            "\n",
            "     nitrogen_dioxide_pollutant_level_was_missing  \\\n",
            "0                                               0   \n",
            "1                                               0   \n",
            "2                                               0   \n",
            "162                                             0   \n",
            "540                                             0   \n",
            "\n",
            "     Average_Ambient_Pressure_was_missing  \\\n",
            "0                                       1   \n",
            "1                                       1   \n",
            "2                                       1   \n",
            "162                                     1   \n",
            "540                                     1   \n",
            "\n",
            "     Average_Ambient_Temperature_was_missing  \\\n",
            "0                                          1   \n",
            "1                                          1   \n",
            "2                                          1   \n",
            "162                                        1   \n",
            "540                                        1   \n",
            "\n",
            "     PM10_pollutant_level_was_missing  PM2.5_pollutant_level_was_missing  \\\n",
            "0                                   1                                  1   \n",
            "1                                   1                                  1   \n",
            "2                                   1                                  0   \n",
            "162                                 1                                  1   \n",
            "540                                 1                                  1   \n",
            "\n",
            "     Light_Absorption_Coeffiecient_was_missing  \n",
            "0                                            1  \n",
            "1                                            1  \n",
            "2                                            1  \n",
            "162                                          1  \n",
            "540                                          1  \n",
            "\n",
            "[5 rows x 27 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REASON:\n",
        "\n",
        "Before imputation, we create missing-value indicator flags for all numerical columns that can be imputed.\n",
        "This allows us to track which values were originally missing, which can be useful for later analysis or modeling.\n",
        "\n",
        "##State-wise Median\n",
        "\n",
        "Daily pollutant and environmental numerical columns are imputed using the median for each state.\n",
        "Median is used because it is robust to outliers, which are common in environmental data.\n",
        "This ensures that the imputed values reflect typical conditions for that state rather than being skewed by extremes.\n",
        "\n",
        "##Global Median\n",
        "\n",
        "As a final safeguard, if any NaNs remain (for example, if an entire state had missing data for a column),\n",
        "we fill those remaining missing values with the global median of that column.\n",
        "This ensures that there are no missing values left in the dataset."
      ],
      "metadata": {
        "id": "25SRD3WZmTUr"
      }
    }
  ]
}